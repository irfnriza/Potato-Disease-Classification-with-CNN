{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63a3266c",
   "metadata": {},
   "source": [
    "# CNN untuk Klasifikasi Penyakit Tanaman Kentang\n",
    "\n",
    "Proyek ini menggunakan Convolutional Neural Network (CNN) murni berbasis PyTorch untuk mengklasifikasikan penyakit pada daun kentang ke dalam 3 kategori: Healthy, Early Blight, dan Late Blight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc79f99d",
   "metadata": {},
   "source": [
    "## 1. Import Libraries dan Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c280d26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python312\\site-packages\\seaborn\\_statistics.py:32: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.4)\n",
      "  from scipy.stats import gaussian_kde\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seed untuk reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a84579",
   "metadata": {},
   "source": [
    "## 2. Data Loading dengan Augmentasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c82d066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 1721 images\n",
      "Validation set: 431 images\n",
      "Test set: 2152 images\n",
      "Classes (3): ['Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy']\n"
     ]
    }
   ],
   "source": [
    "def load_datasets(data_dir, batch_size=32, val_split=0.2, test_split=0.15):\n",
    "    \"\"\"\n",
    "    Load dataset dengan augmentasi on-the-fly untuk training.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Path ke folder dataset\n",
    "        batch_size: Ukuran batch\n",
    "        val_split: Proporsi data untuk validasi\n",
    "        test_split: Proporsi data untuk testing (dari sisa setelah train)\n",
    "    \n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader, test_dataset, num_classes\n",
    "    \"\"\"\n",
    "    # Transformasi untuk training (dengan augmentasi)\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),  # Konversi ke [0,1]\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Transformasi untuk validation dan test (tanpa augmentasi)\n",
    "    eval_transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Load seluruh dataset untuk training/validation/test\n",
    "    full_dataset = datasets.ImageFolder(\n",
    "        root=Path(data_dir),\n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    # Split menjadi train, validation, dan test\n",
    "    total_size = len(full_dataset)\n",
    "    test_size = int(test_split * total_size)\n",
    "    train_val_size = total_size - test_size\n",
    "    val_size = int(val_split * train_val_size)\n",
    "    train_size = train_val_size - val_size\n",
    "    \n",
    "    # Split dataset\n",
    "    train_dataset, val_test_dataset = random_split(\n",
    "        full_dataset, \n",
    "        [train_size + val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    train_dataset, val_dataset_temp = random_split(\n",
    "        train_dataset, \n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    # Buat dataset dengan transform tanpa augmentasi untuk val dan test\n",
    "    full_dataset_eval = datasets.ImageFolder(\n",
    "        root=Path(data_dir),\n",
    "        transform=eval_transform\n",
    "    )\n",
    "    \n",
    "    # Gunakan indices yang sama untuk validasi dan test\n",
    "    val_indices = [train_dataset.indices[i] for i in val_dataset_temp.indices]\n",
    "    test_indices = val_test_dataset.indices\n",
    "    \n",
    "    val_dataset = torch.utils.data.Subset(full_dataset_eval, val_indices)\n",
    "    test_dataset = torch.utils.data.Subset(full_dataset_eval, test_indices)\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                             shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, \n",
    "                           shuffle=False, num_workers=0, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                            shuffle=False, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    num_classes = len(full_dataset.classes)\n",
    "    \n",
    "    print(f\"Dataset split:\")\n",
    "    print(f\"  Total images: {total_size}\")\n",
    "    print(f\"  Train set: {len(train_dataset)} images ({len(train_dataset)/total_size*100:.1f}%)\")\n",
    "    print(f\"  Validation set: {len(val_dataset)} images ({len(val_dataset)/total_size*100:.1f}%)\")\n",
    "    print(f\"  Test set: {len(test_dataset)} images ({len(test_dataset)/total_size*100:.1f}%)\")\n",
    "    print(f\"\\nClasses ({num_classes}): {full_dataset.classes}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, full_dataset_eval, num_classes\n",
    "\n",
    "# Load data - gunakan relative path\n",
    "data_dir = \"../dataset\"\n",
    "train_loader, val_loader, test_loader, test_dataset, num_classes = load_datasets(\n",
    "    data_dir, batch_size=32, val_split=0.2, test_split=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bc8332",
   "metadata": {},
   "source": [
    "## 3. Arsitektur CNN untuk Klasifikasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42e1b313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PotatoCNN(\n",
      "  (conv_blocks): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Dropout2d(p=0.1, inplace=False)\n",
      "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): Dropout2d(p=0.2, inplace=False)\n",
      "    (16): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (21): ReLU(inplace=True)\n",
      "    (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (23): Dropout2d(p=0.3, inplace=False)\n",
      "    (24): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (31): Dropout2d(p=0.4, inplace=False)\n",
      "  )\n",
      "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Total parameters: 1,207,459\n",
      "Trainable parameters: 1,207,459\n"
     ]
    }
   ],
   "source": [
    "class PotatoCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN murni untuk klasifikasi penyakit kentang.\n",
    "    Arsitektur:\n",
    "    - 4 Convolutional blocks dengan BatchNorm dan MaxPooling\n",
    "    - Global Average Pooling\n",
    "    - Fully connected layers dengan Dropout\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(PotatoCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            # Block 1: 3 -> 32 (128x128 -> 64x64)\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.1),\n",
    "            \n",
    "            # Block 2: 32 -> 64 (64x64 -> 32x32)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.2),\n",
    "            \n",
    "            # Block 3: 64 -> 128 (32x32 -> 16x16)\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "            \n",
    "            # Block 4: 128 -> 256 (16x16 -> 8x8)\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.4),\n",
    "        )\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolutional feature extraction\n",
    "        x = self.conv_blocks(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Inisialisasi model\n",
    "model = PotatoCNN(num_classes=num_classes).to(device)\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03b262d",
   "metadata": {},
   "source": [
    "## 4. Training Setup (Loss, Optimizer, Scheduler, Early Stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d74b209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training components initialized:\n",
      "- Loss: CrossEntropyLoss\n",
      "- Optimizer: Adam (lr=1e-3, weight_decay=1e-4)\n",
      "- Scheduler: ReduceLROnPlateau (patience=5, factor=0.5)\n",
      "- Early Stopping: patience=10\n",
      "- Model Checkpoint: best_cnn_model.pth\n"
     ]
    }
   ],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping untuk menghentikan training jika val_loss tidak membaik\"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    \"\"\"Simpan model terbaik berdasarkan validation loss\"\"\"\n",
    "    def __init__(self, filepath, verbose=True):\n",
    "        self.filepath = filepath\n",
    "        self.verbose = verbose\n",
    "        self.best_loss = float('inf')\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'val_loss': val_loss\n",
    "            }, self.filepath)\n",
    "            if self.verbose:\n",
    "                print(f\"Model saved with val_loss: {val_loss:.6f}\")\n",
    "\n",
    "# Setup training components\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', patience=5, factor=0.5\n",
    ")\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=\"../model/best_cnn_model.pth\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Training components initialized:\")\n",
    "print(f\"- Loss: CrossEntropyLoss\")\n",
    "print(f\"- Optimizer: Adam (lr=1e-3, weight_decay=1e-4)\")\n",
    "print(f\"- Scheduler: ReduceLROnPlateau (patience=5, factor=0.5)\")\n",
    "print(f\"- Early Stopping: patience=10\")\n",
    "print(f\"- Model Checkpoint: best_cnn_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3730e4bb",
   "metadata": {},
   "source": [
    "## 5. Fungsi Training dan Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da509629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train untuk satu epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for images, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Predictions\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "    return epoch_loss, epoch_acc.item()\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"Validasi untuk satu epoch\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Predictions\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(val_loader.dataset)\n",
    "    return epoch_loss, epoch_acc.item()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                scheduler, early_stopping, model_checkpoint, \n",
    "                num_epochs=50, device='cpu'):\n",
    "    \"\"\"\n",
    "    Training loop lengkap dengan tracking\n",
    "    \"\"\"\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        # Training\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
    "              f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} | \"\n",
    "              f\"LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # Scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Model checkpoint\n",
    "        model_checkpoint(val_loss, model)\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"Training completed!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Buat folder model jika belum ada\n",
    "Path(\"../model\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32047114",
   "metadata": {},
   "source": [
    "## 6. Mulai Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b160e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Starting training for 100 epochs...\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] | Train Loss: 0.5583 Acc: 0.7908 | Val Loss: 0.3156 Acc: 0.8956 | LR: 0.001000\n",
      "Model saved with val_loss: 0.315623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100] | Train Loss: 0.3937 Acc: 0.8542 | Val Loss: 0.3645 Acc: 0.8608 | LR: 0.001000\n",
      "EarlyStopping counter: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100] | Train Loss: 0.3164 Acc: 0.8675 | Val Loss: 0.5080 Acc: 0.7865 | LR: 0.001000\n",
      "EarlyStopping counter: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100] | Train Loss: 0.3036 Acc: 0.8727 | Val Loss: 0.1794 Acc: 0.9002 | LR: 0.001000\n",
      "Model saved with val_loss: 0.179360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100] | Train Loss: 0.3016 Acc: 0.8861 | Val Loss: 0.1361 Acc: 0.9327 | LR: 0.001000\n",
      "Model saved with val_loss: 0.136123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100] | Train Loss: 0.2618 Acc: 0.8966 | Val Loss: 0.1153 Acc: 0.9582 | LR: 0.001000\n",
      "Model saved with val_loss: 0.115320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100] | Train Loss: 0.2203 Acc: 0.9152 | Val Loss: 0.1004 Acc: 0.9675 | LR: 0.001000\n",
      "Model saved with val_loss: 0.100396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100] | Train Loss: 0.2127 Acc: 0.9134 | Val Loss: 0.1461 Acc: 0.9420 | LR: 0.001000\n",
      "EarlyStopping counter: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100] | Train Loss: 0.2245 Acc: 0.9152 | Val Loss: 0.1059 Acc: 0.9629 | LR: 0.001000\n",
      "EarlyStopping counter: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100] | Train Loss: 0.1914 Acc: 0.9285 | Val Loss: 0.0862 Acc: 0.9814 | LR: 0.001000\n",
      "Model saved with val_loss: 0.086161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100] | Train Loss: 0.2020 Acc: 0.9309 | Val Loss: 0.0626 Acc: 0.9791 | LR: 0.001000\n",
      "Model saved with val_loss: 0.062619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100] | Train Loss: 0.1936 Acc: 0.9268 | Val Loss: 0.0500 Acc: 0.9861 | LR: 0.001000\n",
      "Model saved with val_loss: 0.049966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100] | Train Loss: 0.1578 Acc: 0.9396 | Val Loss: 0.2013 Acc: 0.9211 | LR: 0.001000\n",
      "EarlyStopping counter: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100] | Train Loss: 0.1302 Acc: 0.9570 | Val Loss: 0.0505 Acc: 0.9768 | LR: 0.001000\n",
      "EarlyStopping counter: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100] | Train Loss: 0.1539 Acc: 0.9494 | Val Loss: 0.0769 Acc: 0.9698 | LR: 0.001000\n",
      "EarlyStopping counter: 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100] | Train Loss: 0.1308 Acc: 0.9541 | Val Loss: 0.0341 Acc: 0.9954 | LR: 0.001000\n",
      "Model saved with val_loss: 0.034056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100] | Train Loss: 0.1530 Acc: 0.9448 | Val Loss: 0.0383 Acc: 0.9930 | LR: 0.001000\n",
      "EarlyStopping counter: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100] | Train Loss: 0.1348 Acc: 0.9535 | Val Loss: 0.0357 Acc: 0.9884 | LR: 0.001000\n",
      "EarlyStopping counter: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100] | Train Loss: 0.1048 Acc: 0.9617 | Val Loss: 0.0710 Acc: 0.9722 | LR: 0.001000\n",
      "EarlyStopping counter: 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100] | Train Loss: 0.1185 Acc: 0.9593 | Val Loss: 0.0309 Acc: 0.9954 | LR: 0.001000\n",
      "Model saved with val_loss: 0.030934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100] | Train Loss: 0.1285 Acc: 0.9570 | Val Loss: 0.0251 Acc: 0.9930 | LR: 0.001000\n",
      "Model saved with val_loss: 0.025113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100] | Train Loss: 0.1360 Acc: 0.9483 | Val Loss: 0.0388 Acc: 0.9838 | LR: 0.001000\n",
      "EarlyStopping counter: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100] | Train Loss: 0.0989 Acc: 0.9651 | Val Loss: 0.0269 Acc: 0.9954 | LR: 0.001000\n",
      "EarlyStopping counter: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100] | Train Loss: 0.1171 Acc: 0.9628 | Val Loss: 0.0372 Acc: 0.9884 | LR: 0.001000\n",
      "EarlyStopping counter: 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100] | Train Loss: 0.0892 Acc: 0.9721 | Val Loss: 0.0311 Acc: 0.9954 | LR: 0.001000\n",
      "EarlyStopping counter: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100] | Train Loss: 0.0993 Acc: 0.9698 | Val Loss: 0.0270 Acc: 0.9930 | LR: 0.001000\n",
      "EarlyStopping counter: 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100] | Train Loss: 0.0811 Acc: 0.9750 | Val Loss: 0.0303 Acc: 0.9907 | LR: 0.001000\n",
      "EarlyStopping counter: 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100] | Train Loss: 0.0671 Acc: 0.9785 | Val Loss: 0.0111 Acc: 0.9977 | LR: 0.000500\n",
      "Model saved with val_loss: 0.011054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100] | Train Loss: 0.0664 Acc: 0.9779 | Val Loss: 0.0182 Acc: 0.9977 | LR: 0.000500\n",
      "EarlyStopping counter: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100] | Train Loss: 0.0666 Acc: 0.9814 | Val Loss: 0.0206 Acc: 0.9954 | LR: 0.000500\n",
      "EarlyStopping counter: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100] | Train Loss: 0.0663 Acc: 0.9773 | Val Loss: 0.0297 Acc: 0.9838 | LR: 0.000500\n",
      "EarlyStopping counter: 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/100] | Train Loss: 0.0620 Acc: 0.9802 | Val Loss: 0.0118 Acc: 0.9977 | LR: 0.000500\n",
      "EarlyStopping counter: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100] | Train Loss: 0.0506 Acc: 0.9797 | Val Loss: 0.0397 Acc: 0.9861 | LR: 0.000500\n",
      "EarlyStopping counter: 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100] | Train Loss: 0.0554 Acc: 0.9814 | Val Loss: 0.0129 Acc: 0.9977 | LR: 0.000500\n",
      "EarlyStopping counter: 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/100] | Train Loss: 0.0525 Acc: 0.9826 | Val Loss: 0.0149 Acc: 0.9954 | LR: 0.000250\n",
      "EarlyStopping counter: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100] | Train Loss: 0.0359 Acc: 0.9849 | Val Loss: 0.0092 Acc: 0.9977 | LR: 0.000250\n",
      "Model saved with val_loss: 0.009226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/100] | Train Loss: 0.0402 Acc: 0.9861 | Val Loss: 0.0104 Acc: 0.9977 | LR: 0.000250\n",
      "EarlyStopping counter: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/100] | Train Loss: 0.0401 Acc: 0.9884 | Val Loss: 0.0048 Acc: 1.0000 | LR: 0.000250\n",
      "Model saved with val_loss: 0.004840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/100] | Train Loss: 0.0423 Acc: 0.9866 | Val Loss: 0.0117 Acc: 0.9954 | LR: 0.000250\n",
      "EarlyStopping counter: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▍| 51/54 [01:19<00:04,  1.54s/it]"
     ]
    }
   ],
   "source": [
    "# Mulai training\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    early_stopping=early_stopping,\n",
    "    model_checkpoint=model_checkpoint,\n",
    "    num_epochs=100,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085fa9f3",
   "metadata": {},
   "source": [
    "## 7. Visualisasi Learning Curve dan Akurasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24db82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Visualisasi learning curve, akurasi, dan learning rate\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Plot 1: Loss\n",
    "    axes[0].plot(epochs, history['train_loss'], 'b-o', label='Train Loss', linewidth=2)\n",
    "    axes[0].plot(epochs, history['val_loss'], 'r-o', label='Validation Loss', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].set_title('Learning Curve (Loss)', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(fontsize=11)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Accuracy\n",
    "    axes[1].plot(epochs, history['train_acc'], 'b-o', label='Train Accuracy', linewidth=2)\n",
    "    axes[1].plot(epochs, history['val_acc'], 'r-o', label='Validation Accuracy', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[1].set_title('Learning Curve (Accuracy)', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(fontsize=11)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Learning Rate\n",
    "    axes[2].plot(epochs, history['lr'], 'g-o', linewidth=2)\n",
    "    axes[2].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[2].set_ylabel('Learning Rate', fontsize=12)\n",
    "    axes[2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../model/training_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training Summary:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Best Train Loss: {min(history['train_loss']):.4f}\")\n",
    "    print(f\"Best Val Loss: {min(history['val_loss']):.4f}\")\n",
    "    print(f\"Best Train Accuracy: {max(history['train_acc']):.4f}\")\n",
    "    print(f\"Best Val Accuracy: {max(history['val_acc']):.4f}\")\n",
    "    print(f\"Final Learning Rate: {history['lr'][-1]:.6f}\")\n",
    "    print(f\"Total Epochs: {len(history['train_loss'])}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47db00d8",
   "metadata": {},
   "source": [
    "## 8. Load Model Terbaik dan Evaluasi pada Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eaa3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model terbaik\n",
    "checkpoint = torch.load(\"../model/best_cnn_model.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded best model with validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluasi model pada test set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels), np.array(all_probs)\n",
    "\n",
    "# Evaluasi\n",
    "predictions, ground_truth, probabilities = evaluate_model(model, test_loader, device)\n",
    "print(f\"\\nEvaluated {len(predictions)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badcb5c1",
   "metadata": {},
   "source": [
    "## 9. Confusion Matrix dan Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f77134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "def print_classification_metrics(predictions, ground_truth, class_names):\n",
    "    \"\"\"\n",
    "    Print detailed classification metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Classification Metrics\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Overall accuracy\n",
    "    accuracy = accuracy_score(ground_truth, predictions)\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    print(f\"\\nPer-Class Accuracy:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_mask = ground_truth == i\n",
    "        class_acc = np.mean(predictions[class_mask] == ground_truth[class_mask])\n",
    "        print(f\"  {class_name}: {class_acc:.4f} ({class_acc*100:.2f}%)\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Detailed Classification Report:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(classification_report(ground_truth, predictions, target_names=class_names, digits=4))\n",
    "\n",
    "# Print metrics\n",
    "class_names = test_dataset.classes\n",
    "print_classification_metrics(predictions, ground_truth, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c67430b",
   "metadata": {},
   "source": [
    "## 10. Visualisasi Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6714a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(predictions, ground_truth, class_names):\n",
    "    \"\"\"\n",
    "    Visualisasi confusion matrix\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(ground_truth, predictions)\n",
    "    \n",
    "    # Normalize confusion matrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Raw counts\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                ax=axes[0], cbar_kws={'label': 'Count'})\n",
    "    axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
    "    axes[0].set_ylabel('True Label', fontsize=12)\n",
    "    axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Normalized (percentages)\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                ax=axes[1], cbar_kws={'label': 'Percentage'})\n",
    "    axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
    "    axes[1].set_ylabel('True Label', fontsize=12)\n",
    "    axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../model/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(predictions, ground_truth, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cba396b",
   "metadata": {},
   "source": [
    "## 11. Visualisasi Prediksi: Benar vs Salah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4924a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(test_loader, model, class_names, device, n_correct=5, n_wrong=5):\n",
    "    \"\"\"\n",
    "    Visualisasi prediksi yang benar dan salah\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Denormalize function\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    def denormalize(img):\n",
    "        img = img.numpy().transpose((1, 2, 0))\n",
    "        img = std * img + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "        return img\n",
    "    \n",
    "    correct_images = []\n",
    "    wrong_images = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images_gpu = images.to(device)\n",
    "            labels_gpu = labels.to(device)\n",
    "            \n",
    "            outputs = model(images_gpu)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            for i in range(len(labels)):\n",
    "                if len(correct_images) >= n_correct and len(wrong_images) >= n_wrong:\n",
    "                    break\n",
    "                \n",
    "                img = images[i].cpu()\n",
    "                label = labels[i].item()\n",
    "                pred = preds[i].item()\n",
    "                prob = probs[i].cpu().numpy()\n",
    "                \n",
    "                if pred == label and len(correct_images) < n_correct:\n",
    "                    correct_images.append((img, label, pred, prob))\n",
    "                elif pred != label and len(wrong_images) < n_wrong:\n",
    "                    wrong_images.append((img, label, pred, prob))\n",
    "            \n",
    "            if len(correct_images) >= n_correct and len(wrong_images) >= n_wrong:\n",
    "                break\n",
    "    \n",
    "    # Plot correct predictions\n",
    "    fig, axes = plt.subplots(2, n_correct, figsize=(n_correct * 3, 6))\n",
    "    \n",
    "    for i, (img, label, pred, prob) in enumerate(correct_images):\n",
    "        ax = axes[0, i] if n_correct > 1 else axes[0]\n",
    "        ax.imshow(denormalize(img))\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'True: {class_names[label]}\\nPred: {class_names[pred]}\\nConf: {prob[pred]:.2%}',\n",
    "                    fontsize=10, color='green', fontweight='bold')\n",
    "    \n",
    "    if n_correct > 1:\n",
    "        axes[0, 0].set_ylabel('CORRECT\\nPREDICTIONS', fontsize=12, fontweight='bold', color='green')\n",
    "    \n",
    "    # Plot wrong predictions\n",
    "    for i, (img, label, pred, prob) in enumerate(wrong_images):\n",
    "        ax = axes[1, i] if n_wrong > 1 else axes[1]\n",
    "        ax.imshow(denormalize(img))\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'True: {class_names[label]}\\nPred: {class_names[pred]}\\nConf: {prob[pred]:.2%}',\n",
    "                    fontsize=10, color='red', fontweight='bold')\n",
    "    \n",
    "    if n_wrong > 1:\n",
    "        axes[1, 0].set_ylabel('WRONG\\nPREDICTIONS', fontsize=12, fontweight='bold', color='red')\n",
    "    \n",
    "    plt.suptitle('Sample Predictions: Correct vs Wrong', fontsize=14, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../model/prediction_samples.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions(test_loader, model, class_names, device, n_correct=5, n_wrong=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95b8681",
   "metadata": {},
   "source": [
    "## 12. Visualisasi Prediksi per Kelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a113246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions_per_class(test_loader, model, class_names, device, n_samples=4):\n",
    "    \"\"\"\n",
    "    Visualisasi beberapa prediksi untuk setiap kelas\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Denormalize function\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    def denormalize(img):\n",
    "        img = img.numpy().transpose((1, 2, 0))\n",
    "        img = std * img + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "        return img\n",
    "    \n",
    "    # Collect samples for each class\n",
    "    class_samples = {i: [] for i in range(len(class_names))}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images_gpu = images.to(device)\n",
    "            \n",
    "            outputs = model(images_gpu)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i].item()\n",
    "                if len(class_samples[label]) < n_samples:\n",
    "                    img = images[i].cpu()\n",
    "                    pred = preds[i].item()\n",
    "                    prob = probs[i].cpu().numpy()\n",
    "                    class_samples[label].append((img, label, pred, prob))\n",
    "            \n",
    "            if all(len(samples) >= n_samples for samples in class_samples.values()):\n",
    "                break\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(len(class_names), n_samples, figsize=(n_samples * 3, len(class_names) * 3))\n",
    "    \n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        for i, (img, label, pred, prob) in enumerate(class_samples[class_idx]):\n",
    "            ax = axes[class_idx, i] if len(class_names) > 1 else axes[i]\n",
    "            ax.imshow(denormalize(img))\n",
    "            ax.axis('off')\n",
    "            \n",
    "            is_correct = label == pred\n",
    "            color = 'green' if is_correct else 'red'\n",
    "            title = f'Pred: {class_names[pred]}\\nConf: {prob[pred]:.2%}'\n",
    "            ax.set_title(title, fontsize=10, color=color, fontweight='bold')\n",
    "            \n",
    "            if i == 0:\n",
    "                ax.set_ylabel(f'{class_name}\\n(True Label)', \n",
    "                            fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('Sample Predictions per Class', fontsize=14, fontweight='bold', y=0.99)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../model/predictions_per_class.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions_per_class(test_loader, model, class_names, device, n_samples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7eb560",
   "metadata": {},
   "source": [
    "## 13. Ringkasan Akhir dan Kesimpulan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59cf2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def print_final_summary(predictions, ground_truth, class_names, history):\n",
    "    \"\"\"\n",
    "    Cetak ringkasan akhir dari seluruh eksperimen\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" \" * 25 + \"FINAL SUMMARY - CNN PROJECT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n### DATASET ###\")\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    print(f\"Total test images: {len(predictions)}\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        count = np.sum(ground_truth == i)\n",
    "        print(f\"  - {class_name}: {count} images\")\n",
    "    \n",
    "    print(\"\\n### TRAINING ###\")\n",
    "    print(f\"Total epochs trained: {len(history['train_loss'])}\")\n",
    "    print(f\"Best training loss: {min(history['train_loss']):.4f}\")\n",
    "    print(f\"Best validation loss: {min(history['val_loss']):.4f}\")\n",
    "    print(f\"Best training accuracy: {max(history['train_acc']):.4f}\")\n",
    "    print(f\"Best validation accuracy: {max(history['val_acc']):.4f}\")\n",
    "    print(f\"Final learning rate: {history['lr'][-1]:.6f}\")\n",
    "    \n",
    "    print(\"\\n### MODEL ARCHITECTURE ###\")\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Model type: CNN (Custom Architecture)\")\n",
    "    \n",
    "    print(\"\\n### TEST SET PERFORMANCE ###\")\n",
    "    accuracy = accuracy_score(ground_truth, predictions)\n",
    "    \n",
    "    # Macro-averaged metrics\n",
    "    precision = precision_score(ground_truth, predictions, average='macro', zero_division=0)\n",
    "    recall = recall_score(ground_truth, predictions, average='macro', zero_division=0)\n",
    "    f1 = f1_score(ground_truth, predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    print(f\"Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"Macro Precision: {precision:.4f}\")\n",
    "    print(f\"Macro Recall: {recall:.4f}\")\n",
    "    print(f\"Macro F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Class Performance:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_mask = ground_truth == i\n",
    "        class_acc = np.mean(predictions[class_mask] == ground_truth[class_mask])\n",
    "        class_precision = precision_score(ground_truth == i, predictions == i, zero_division=0)\n",
    "        class_recall = recall_score(ground_truth == i, predictions == i, zero_division=0)\n",
    "        class_f1 = f1_score(ground_truth == i, predictions == i, zero_division=0)\n",
    "        \n",
    "        print(f\"  {class_name}:\")\n",
    "        print(f\"    Accuracy: {class_acc:.4f} | Precision: {class_precision:.4f} | \"\n",
    "              f\"Recall: {class_recall:.4f} | F1: {class_f1:.4f}\")\n",
    "    \n",
    "    print(\"\\n### OUTPUT FILES ###\")\n",
    "    print(\"✓ Model: ../model/best_cnn_model.pth\")\n",
    "    print(\"✓ Training history: ../model/training_history.png\")\n",
    "    print(\"✓ Confusion matrix: ../model/confusion_matrix.png\")\n",
    "    print(\"✓ Prediction samples: ../model/prediction_samples.png\")\n",
    "    print(\"✓ Predictions per class: ../model/predictions_per_class.png\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" \" * 30 + \"PROJECT COMPLETED!\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print_final_summary(predictions, ground_truth, class_names, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fba7f3",
   "metadata": {},
   "source": [
    "## 14. Fungsi Utilitas untuk Prediksi Gambar Baru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03021645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fungsi predict_single_image() siap digunakan!\n",
      "Uncomment kode di atas untuk mencoba prediksi pada gambar tunggal.\n"
     ]
    }
   ],
   "source": [
    "def predict_single_image(image_path, model, class_names, device):\n",
    "    \"\"\"\n",
    "    Prediksi untuk gambar tunggal\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path ke gambar\n",
    "        model: Model CNN yang sudah dilatih\n",
    "        class_names: List nama kelas\n",
    "        device: Device (cuda/cpu)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary berisi hasil prediksi\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "    \n",
    "    # Load dan preprocess gambar\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Denormalize function untuk visualisasi\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    def denormalize(img):\n",
    "        img = img.numpy().transpose((1, 2, 0))\n",
    "        img = std * img + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "        return img\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Prediksi\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        confidence, pred_idx = torch.max(probs, 1)\n",
    "        \n",
    "        pred_idx = pred_idx.item()\n",
    "        confidence = confidence.item()\n",
    "        all_probs = probs[0].cpu().numpy()\n",
    "    \n",
    "    # Visualisasi\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Original image (denormalized)\n",
    "    axes[0].imshow(denormalize(image_tensor.cpu().squeeze()))\n",
    "    axes[0].set_title('Input Image', fontweight='bold', fontsize=12)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Prediction probabilities bar chart\n",
    "    colors = ['green' if i == pred_idx else 'lightgray' for i in range(len(class_names))]\n",
    "    bars = axes[1].barh(class_names, all_probs, color=colors)\n",
    "    axes[1].set_xlabel('Probability', fontsize=12)\n",
    "    axes[1].set_title('Class Probabilities', fontweight='bold', fontsize=12)\n",
    "    axes[1].set_xlim([0, 1])\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    for i, (bar, prob) in enumerate(zip(bars, all_probs)):\n",
    "        axes[1].text(prob + 0.02, bar.get_y() + bar.get_height()/2, \n",
    "                    f'{prob:.2%}', va='center', fontsize=10,\n",
    "                    fontweight='bold' if i == pred_idx else 'normal')\n",
    "    \n",
    "    fig.suptitle(f'Prediction: {class_names[pred_idx]} (Confidence: {confidence:.2%})', \n",
    "                 fontsize=14, fontweight='bold', color='green')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'predicted_class': class_names[pred_idx],\n",
    "        'predicted_index': pred_idx,\n",
    "        'confidence': confidence,\n",
    "        'all_probabilities': {class_names[i]: all_probs[i] for i in range(len(class_names))}\n",
    "    }\n",
    "\n",
    "# Contoh penggunaan (uncomment untuk mencoba):\n",
    "# result = predict_single_image(\n",
    "#     \"../dataset/Potato___healthy/<nama_file>.JPG\",\n",
    "#     model, \n",
    "#     class_names,\n",
    "#     device\n",
    "# )\n",
    "# print(result)\n",
    "\n",
    "print(\"Fungsi predict_single_image() siap digunakan!\")\n",
    "print(\"Uncomment kode di atas untuk mencoba prediksi pada gambar tunggal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43f89be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
